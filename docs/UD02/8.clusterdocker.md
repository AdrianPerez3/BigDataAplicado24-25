# Cluster pseudodistribuido en docker


> [!ERROR] NO DISPONIBLE
> Contenido pendiente de finalizar y revisar

## Opción 1

```bash
docker run -ti -p 8042 -p 8088 -p 19888 -p 50070 -p 50075 harisekhon/hadoop
```

## Revisar 1
https://medium.com/@bayuadiwibowo/deploying-a-big-data-ecosystem-dockerized-hadoop-spark-hive-and-zeppelin-654014069c82

## Revisar 2
https://www.writecode.es/2019-02-25-cluster_hadoop_docker/


!!!danger
	**SIN TERMINAR**

* Lo vamos a hacer sobre debian 11 slim (debian 11 tiene java 11, debian 12 java 17).
* Creamos una carpeta para preparar la imagen y allí creamos el archivo Dockerfile.

## Creación imagen debian 11

```bash
#En una pestaña del terminal corremos un contenedor con debian 11 slim para ir probando los comandos

#Indico la plataforma porque estoy con macos m1

## Hago un pull
docker pull --platform linux/amd64 debian:11-slim
## Y después un run
docker run --platform linux/amd64 -it debian:11-slim

```

En otra pestaña nos ponemos con el Dockerfile, creamos el archivo con el siguiente contenido, por el momento una debian 11, con locales en español, actualizada y con algunos paquetes instalados

```dockerfile
FROM debian:11-slim

# Configuración de entorno
ENV LANG=es_ES.UTF-8 \
    LANGUAGE=es_ES:es \
    LC_ALL=es_ES.UTF-8

# Instalación de dependencias
RUN apt-get update && apt-get install -y \
    locales openjdk-11-jdk net-tools curl netcat gnupg libsnappy-dev vim \
    && rm -rf /var/lib/apt/lists/* \
    && localedef -i es_ES -c -f UTF-8 -A /usr/share/locale/locale.alias es_ES.UTF-8

```

Podemos hacer un build del Dockerfile que tenemos para ver cómo va quedando:

```
docker build -t hadoop-base-image --platform linux/amd64 .
```

Ya tenemos una imagen creada según el Dockerfile

```bash
docker image ls

REPOSITORY                     TAG       IMAGE ID       CREATED          SIZE
hadoop-base-image              latest    4bfe812532fc   6 minutes ago    668MB

```

Podemos "arrancarla", en modo interactivo (-it), cuando salimos se borra lo que hemos hecho
```
docker run --platform linux/amd64 -it hadoop-base-image
```

## Descarga e instalación hadoop

Modificamos Dockerfile con:

```Dockerfile

FROM debian:11-slim

# Configuración de entorno
ENV LANG=es_ES.UTF-8 \
    LANGUAGE=es_ES:es \
    LC_ALL=es_ES.UTF-8 \
    HADOOP_VERSION=3.4.0 \
    HADOOP_HOME=/opt/hadoop

# Instalación de dependencias
RUN apt-get update && apt-get install -y \
    locales openjdk-11-jdk net-tools curl netcat gnupg libsnappy-dev vim \
    && rm -rf /var/lib/apt/lists/* \
    && localedef -i es_ES -c -f UTF-8 -A /usr/share/locale/locale.alias es_ES.UTF-8

# Descarga e instalación de Hadoop
# No hacemos ningún enlace, nos descargamos hadoop
# lo descomprimimos en OPT y renombramos a hadoop
RUN curl -O https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} $HADOOP_HOME && \
    rm hadoop-${HADOOP_VERSION}.tar.gz


# Configuración de variables de entorno para Hadoop y Java
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \
    JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Configuración básica de Hadoop (core-site.xml y hdfs-site.xml)
RUN echo '<?xml version="1.0"?> \
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?> \
    <configuration> \
        <property> \
            <name>fs.defaultFS</name> \
            <value>hdfs://localhost:9000</value> \
        </property> \
    </configuration>' > $HADOOP_HOME/etc/hadoop/core-site.xml && \
    echo '<?xml version="1.0"?> \
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?> \
    <configuration> \
        <property> \
            <name>dfs.replication</name> \
            <value>1</value> \
        </property> \
    </configuration>' > $HADOOP_HOME/etc/hadoop/hdfs-site.xml

# Crear directorios de Hadoop necesarios para almacenamiento de datos
RUN mkdir -p /opt/hadoop/hdfs/namenode && \
    mkdir -p /opt/hadoop/hdfs/datanode

# Configuración del puerto para Hadoop (en caso de necesitar mapear)
EXPOSE 9870 9000

# Comando de inicio para HDFS
CMD ["bash"]

```

Reconstruimos de nuevo la imagen:
```
docker build -t hadoop-base-image --platform linux/amd64 .
```

Y la levantamos:
```
docker run --platform linux/amd64 -it hadoop-base-image
```

Y devuelve el siguiente error:
```
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at facb9e116dc3/192.168.215.2
************************************************************/
Iniciando HDFS...
Starting namenodes on [localhost]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [facb9e116dc3]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
```

## Última corrección
Este error ocurre porque Hadoop no está diseñado para ejecutarse como root y necesita usuarios específicos definidos para ejecutar los procesos de namenode, datanode, y secondarynamenode.

Para solucionarlo, puedes crear un usuario no-root en el Dockerfile y configurar las variables de entorno:
HDFS_NAMENODE_USER
HDFS_DATANODE_USER
HDFS_SECONDARYNAMENODE_USER

```dockerfile

FROM debian:11-slim

ENV LANG=es_ES.UTF-8 \
    LANGUAGE=es_ES:es \
    LC_ALL=es_ES.UTF-8 \
    HADOOP_VERSION=3.4.0 \
    HADOOP_HOME=/opt/hadoop \
    HDFS_NAMENODE_USER=hadoop \
    HDFS_DATANODE_USER=hadoop \
    HDFS_SECONDARYNAMENODE_USER=hadoop

# Instalación de dependencias
RUN apt-get update && apt-get install -y \
    locales openjdk-11-jdk net-tools curl netcat-openbsd gnupg libsnappy-dev vim openssh-client openssh-server \
    && rm -rf /var/lib/apt/lists/* \
    && localedef -i es_ES -c -f UTF-8 -A /usr/share/locale/locale.alias es_ES.UTF-8

# Crear usuario no-root para Hadoop
RUN useradd -m hadoop

# Descargar e instalar Hadoop
RUN curl -O https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} $HADOOP_HOME && \
    rm hadoop-${HADOOP_VERSION}.tar.gz && \
    chown -R hadoop:hadoop $HADOOP_HOME

# Configuración de variables de entorno para Hadoop y Java
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin \
    JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Configuración básica de Hadoop
RUN echo '<?xml version="1.0"?> \
    <configuration> \
        <property> \
            <name>fs.defaultFS</name> \
            <value>hdfs://localhost:9000</value> \
        </property> \
    </configuration>' > $HADOOP_HOME/etc/hadoop/core-site.xml && \
    echo '<?xml version="1.0"?> \
    <configuration> \
        <property> \
            <name>dfs.replication</name> \
            <value>1</value> \
        </property> \
    </configuration>' > $HADOOP_HOME/etc/hadoop/hdfs-site.xml

# Crear directorios de Hadoop necesarios y asignar permisos al usuario
RUN mkdir -p /opt/hadoop/hdfs/namenode && \
    mkdir -p /opt/hadoop/hdfs/datanode && \
    chown -R hadoop:hadoop /opt/hadoop/hdfs

# Configurar SSH para que funcione sin clave en localhost
RUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N '' && \
    ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -q -N "" && \
    cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys && \
    chmod 600 /home/hadoop/.ssh/authorized_keys && \
    chown -R hadoop:hadoop /home/hadoop/.ssh
    
# Script de inicio para formateo y arranque
RUN echo '#!/bin/bash \n\
if [ ! -d "/opt/hadoop/hdfs/namenode/current" ]; then \n\
    echo "Formateando namenode..." \n\
    $HADOOP_HOME/bin/hdfs namenode -format \n\
fi \n\
echo "Iniciando HDFS..." \n\
$HADOOP_HOME/sbin/start-dfs.sh \n\
tail -f /dev/null' > /opt/start-hadoop.sh && chmod +x /opt/start-hadoop.sh

# Cambiar al usuario hadoop
USER hadoop

# Establecer script de inicio
CMD ["/opt/start-hadoop.sh"]
```

Reconstruimos la imagen y la levantamos:
```
docker build -t hadoop-base-image --platform linux/amd64 .


```

