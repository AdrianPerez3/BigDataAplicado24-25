# Ejercicios hdfs

Podéis encontrar los archivos de datos en el siguiente repositorio:
https://github.com/josepgarcia/datos
### Ejercicio 1. Fichero de logs.
- Descargamos un fichero más grande (access_log.gz )
- Lo descomprimimos. Quedará un archivo de 482mb.
- Lo movemos a HDFS, dentro la carpeta 
- Comprobamos en cuantos bloques se encuentra.
- ¿A qué archivos apunta en la máquina virtual?

### Ejercicio 2
1. Crea un fichero “saludo.txt” en local, que contenga el texto “Hola”.
   Súbelo a HDFS, a la carpeta /temporal (si no existe hay que crearla)
   Borra el fichero en local
   Muestra el contenido del fichero (en remoto).
2. Copia el fichero saludo.txt a local con el nombre (saludolocal.txt)
3. Entra a la web de administración para ver que existe el fichero.
4. Borra el fichero remoto.
5. Asegúrate que se ha borrado el fichero con ls.
6. Borra el directorio temporal.
```bash
$ hdfs dfs -cat /temporal/prueba.txt 
$ hdfs dfs -get /temporal/prueba.txt /tmp/borrar.txt 
$ hdfs dfs -mkdir directorio1 $ hdfs dfs -rm /temporal/prueba.txt
```

### Ejercicio 3
1. Crea un fichero “otrosaludo.txt” en local, que contenga el texto “Hola”.
   **MUÉVELO** a HDFS, dentro de la carpeta /ejercicios/saludos/
   Comprueba que ya no existe el fichero en local
2. Crea un directorio en local llamado prueba
   Dentro de este directorio crea un fichero llamado ejercicioprueba.txt
   Mueve todo el directorio prueba a HDFS, dentro de la carpeta /ejercicios
   Comprueba que ya no existe la carpeta en local
   Realiza una copia de HDFS a local de la carpeta que acabas de subir.
```bash
# Move file / Folder from Local disk to HDFS 
$ hdfs dfs -moveFromLocal /local-file-path /hdfs-file-path 
# Copy 
$ hdfs dfs -copyFromLocal /hdfs-file-path /local-file-path 
# Move a File to HDFS from Local 
$ hdfs dfs -moveToLocal /hdfs-file-path /local-file-path 
# Copy 
$ hdfs dfs -copyToLocal /hdfs-file-path /local-file-path
```
[https://sparkbyexamples.com/apache-hadoop/hadoop-hdfs-dfs-commands-and-starting-hdfs-dfs-services/](https://sparkbyexamples.com/apache-hadoop/hadoop-hdfs-dfs-commands-and-starting-hdfs-dfs-services/)
### Ejercicio 4
1. Crea un archivo en /tmp llamado archivogrande que tenga un tamaño de 500MB (aproximadamente) 
   -> Utiliza el comando dd
2. Crea una carpeta en HDFS llamada datos2.
3. Sube el archivo a la carpeta creada.
```bash
# Crear archivo de 500Mb 
dd if=/dev/zero of=/tmp/archivogrande bs=1024 count=512k 
# Verificar el tamaño del archivo creado 
du -sh /tmp/archivogrande
```
