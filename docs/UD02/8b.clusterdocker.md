# Versión 2 clúster docker

> [!INFO] Hadoop 3.4.1 + Java 11
> En preparación


Archivo: `Dockerfile`
```dockerfile

# Imagen base
FROM openjdk:11-jdk

# Establecer variables de entorno
ENV HADOOP_VERSION=3.4.1
ENV HADOOP_HOME=/opt/hadoop
ENV JAVA_HOME=/usr/local/openjdk-11
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Instalar paquetes necesarios
RUN apt-get update && \
    apt-get install -y openssh-server curl vim wget rsync && \
    rm -rf /var/lib/apt/lists/* 
##   && localedef -i es_ES -c -f UTF-8 -A /usr/share/locale/locale.alias es_ES.UTF-8

# Crear usuario y grupo hadoop
RUN groupadd hadoop && \
    useradd -ms /bin/bash -g hadoop hadoop

# Instalar sudo
RUN apt-get update && apt-get install -y sudo && rm -rf /var/lib/apt/lists/*

# Permitir que el usuario hadoop use sudo sin contraseña
RUN echo "hadoop ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Configurar SSH para el usuario hadoop
RUN mkdir -p /home/hadoop/.ssh && \
    ssh-keygen -t rsa -f /home/hadoop/.ssh/id_rsa -q -N "" && \
    cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys && \
    chown -R hadoop:hadoop /home/hadoop/.ssh && \
    chmod 600 /home/hadoop/.ssh/authorized_keys

# Instalar Hadoop
#COPY hadoop-3.3.5.tar.gz /tmp/
#RUN tar -xzvf /tmp/hadoop-3.3.5.tar.gz -C /usr/local/ && \
    #mv /usr/local/hadoop-3.3.5 $HADOOP_HOME && \
    #rm /tmp/hadoop-3.3.5.tar.gz && \
    #chown -R hadoop:hadoop $HADOOP_HOME

# Descargamos hadoop para instalarlo
RUN curl -O https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} $HADOOP_HOME && \
    rm hadoop-${HADOOP_VERSION}.tar.gz && \
    chown -R hadoop:hadoop $HADOOP_HOME


# Configurar variables de entorno de Hadoop
RUN echo "export JAVA_HOME=/usr/local/openjdk-11" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Copiar archivos de configuración
COPY config/* $HADOOP_HOME/etc/hadoop/
#RUN chown -R hadoop:hadoop $HADOOP_HOME/etc/hadoop/
RUN chown -R hadoop:hadoop $HADOOP_HOME

# Cambiar al usuario hadoop
USER hadoop

# Formatear HDFS
RUN $HADOOP_HOME/bin/hdfs namenode -format

# Exponer puertos
EXPOSE 9870 8088 9000 8042 22

# Volver al usuario root para copiar el script de inicio
USER root

# Copiar el script de inicio
COPY start-hadoop.sh /start-hadoop.sh
RUN chmod +x /start-hadoop.sh

# Cambiar al usuario hadoop
USER hadoop

# Definir el punto de entrada
CMD ["/start-hadoop.sh"]


```

Archivo: `start-hadoop.sh`
```bash
#!/bin/bash

# Iniciar el servicio SSH (requiere privilegios de root)
sudo service ssh start

# Iniciar los servicios de Hadoop
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh

# Mantener el contenedor en ejecución
tail -f /dev/null
```
