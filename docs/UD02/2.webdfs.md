# UD02 2. webDFS

~~Hadoop 2 → Puerto 50070~~

Hadoop 3 → Puerto 9870

Podemos acceder a la web de administración a través de:

http://HOSTNAME_O_IP:PUERTO

[http://192.168.64.14:9870/](http://192.168.64.14:9870/)

[http://debianh:9864/](http://debianh:9864/) → Menos opciones

![Untitled](<./_PENDENT/UD02 2 webDFS 128e913de6c481538d4ad2b9038f4cf4/Untitled.png>)

![SCR-20231026-jdby.png](<./_PENDENT/UD02 2 webDFS 128e913de6c481538d4ad2b9038f4cf4/SCR-20231026-jdby.png>)

![SCR-20231026-jerd.png](<./_PENDENT/UD02 2 webDFS 128e913de6c481538d4ad2b9038f4cf4/SCR-20231026-jerd.png>)

![Untitled](<./_PENDENT/UD02 2 webDFS 128e913de6c481538d4ad2b9038f4cf4/Untitled 1.png>)

Replication 1 (solo tenemos 1 nodo)

Tamaño de bloque 128MB

Si pinchamos en prueba.txt vemos la información del fichero, si fuese muy grande mostraría en qué nodos se encuentra

En el sistema lo podemos ver en /datos/datenode/current/BP-*/current/finaliced….

Cuando subimos un fichero lo trocea y lo guarda en ficheros con nombres de este estilo.

![Untitled](<./_PENDENT/UD02 2 webDFS 128e913de6c481538d4ad2b9038f4cf4/Untitled 2.png>)

<aside>
⚠️ **ERROR - ERROR - ERROR - ERROR - ERROR - ERROR - ERROR - ERROR**

La  versión 17 de openjdk da error con `webdfs`.
Utilizar java 11:
[https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb](https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb)

</aside>

<aside>
ℹ️ **¿Cómo desinstalar java 17 e instalar java 11?**

1. Apagar hadoop

2. Apagar MV

3. Clonar máquina virtual, llamar a la nueva algo así como: hadoop_java11 (para poder diferenciarla fácilmente)

→ Clonamos la máquina virtual para tener una copia por si hay algún problema en instalación de Java11

4. Iniciar máquina virtual nueva

5. Desinstalar java 17

- ¿Cómo desinstalar java?
    
    ```bash
    # apt remove openjdk*
    apt purge openjdk-17*
    ```
    

6. Descargar java 11, para no tener que registrarte en oracle, puedes descargar java 11 desde la siguiente URL:

[https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb](https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb)

7. Instalar el paquete que te has descargado

- ¿Cómo instalar paquetes .deb en debian?
    
    ```bash
    dpkg -i [jdk-11.0.20_linux-x64_bin.deb](https://cfdownload.adobe.com/pub/adobe/coldfusion/java/java11/java11020/jdk-11.0.20_linux-x64_bin.deb)
    ```
    

8. Enceder hadoop 

→ dará error, anteriormente en la configuración, hemos indicado la ruta de java en algunos archivos:

Por ejemplo, en .bashrc añadimos la siguiente línea:

```bash
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
```

    Hemos de:

- Averiguar la ruta del nuevo java que hemos instalado (está dentro de /usr/lib/jvm/ también).
- Reemplazarla en los archivos de configuración necesarios (lo hemos hecho en la primera práctica, la de instalación de hadoop).
- ¿Qué archivos hay que modificar?
    
    hadoop-env.sh, .bashrc
    * Para que se apliquen las moficicaciones en .bashrc hay que cerrar sesión y volver a abrir o ejecutar `source .bashrc`
    

9. Encender hadoop, ahora debería funcionar.

10. Comprobar que funciona web hdfs file explorer

</aside>

# Errores

## Error de escritura web ui

Si intentamos crear una carpeta o eliminar algún archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode="/":iabd:supergroup:drwxr-xr-x. Por defecto, los recursos via web los crea el usuario dr.who.

![Untitled](<./_PENDENT/UD02 2 webDFS 128e913de6c481538d4ad2b9038f4cf4/Untitled 3.png>)

Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta:

```bash
hdfs dfs -mkdir /pruebas
hdfs dfs -chmod 777 /pruebas
```

Si ahora accedemos al interfaz, sí que podremos trabajar con la carpeta `pruebas` via web, teniendo en cuenta que las operaciones las realiza el usuario `dr.who` que pertenece al grupo `supergroup`.

Otra posibilidad es modificar el archivo de configuración `core-site.xml` y añadir una propiedad para modificar el usuario estático:

```bash
<property>
    <name>hadoop.http.staticuser.user</name>
    <value>hadoop</value>
</property>
```

Tras reiniciar *Hadoop*, ya podremos crear los recursos como el usuario hadoop.